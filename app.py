import streamlit as st`nfrom supabase import create_client, Client`nfrom langchain_community.vectorstores import SupabaseVectorStore`nfrom langchain_openai import OpenAIEmbeddings, ChatOpenAI`nfrom langchain.chains import RetrievalQA`nfrom langchain.prompts import PromptTemplate`nimport os`n`n# --- Configuración de la Página ---`nst.set_page_config(page_title="Semhys AI", page_icon="", layout="wide")`n`n# Ocultar menú y footer de Streamlit para apariencia limpia`nhide_streamlit_style = """`n<style>`n#MainMenu {visibility: hidden;}`nfooter {visibility: hidden;}`nheader {visibility: hidden;}`n</style>`n"""`nst.markdown(hide_streamlit_style, unsafe_allow_html=True)`n`n# --- Inicialización ---`n# Verificar secretos (para desarrollo local o deploy)`ntry:`n    SUPABASE_URL = st.secrets["SUPABASE_URL"]`n    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]`n    OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]`nexcept FileNotFoundError:`n    st.error("Error: No se encontraron los secretos. Asegúrate de configurar .streamlit/secrets.toml")`n    st.stop()`n`n# Cliente Supabase`nsupabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)`n`n# Embeddings y Vector Store`nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)`nvector_store = SupabaseVectorStore(`n    client=supabase,`n    embedding=embeddings,`n    table_name="documents",`n    query_name="match_documents"`n)`n`n# Modelo LLM`nllm = ChatOpenAI(`n    model_name="gpt-4", # O "gpt-3.5-turbo" según presupuesto`n    temperature=0.3,`n    openai_api_key=OPENAI_API_KEY`n)`n`n# Prompt Personalizado`nsystem_template = """`nEres el Ingeniero Senior de Semhys. Tu misión es proporcionar asistencia técnica experta.`nResponde consultas sobre hidráulica, electricidad, mantenimiento y servicios de ingeniería basándote SOLO en el contexto proporcionado a continuación.`nSi la respuesta no se encuentra en el contexto, responde honestamente que no tienes esa información y ofrece contactar a un especialista humano de Semhys.`nSé profesional, conciso y técnico pero accesible.`n`nContexto:`n{context}`n`nPregunta:`n{question}`n`nRespuesta del Ingeniero:`n"""`nPROMPT = PromptTemplate(`n    template=system_template, input_variables=["context", "question"]`n)`n`n# Chain de Retrieval QA`nqa_chain = RetrievalQA.from_chain_type(`n    llm=llm,`n    chain_type="stuff",`n    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),`n    chain_type_kwargs={"prompt": PROMPT}`n)`n`n# --- Interfaz de Chat ---`nst.title("Asistente Técnico Semhys ")`nst.caption("Especialista en Mantenimiento e Ingeniería")`n`n# Inicializar historial`nif "messages" not in st.session_state:`n    st.session_state.messages = []`n`n# Mostrar mensajes previos`nfor message in st.session_state.messages:`n    with st.chat_message(message["role"]):`n        st.markdown(message["content"])`n`n# Entrada de usuario`nif prompt := st.chat_input("Escribe tu consulta técnica aquí..."):`n    # Guardar y mostrar mensaje usuario`n    st.session_state.messages.append({"role": "user", "content": prompt})`n    with st.chat_message("user"):`n        st.markdown(prompt)`n`n    # Generar respuesta`n    with st.chat_message("assistant"):`n        message_placeholder = st.empty()`n        full_response = ""`n        `n        try:`n            with st.spinner("Analizando base de datos técnica..."):`n                result = qa_chain.run(prompt)`n                full_response = result`n                message_placeholder.markdown(full_response)`n        except Exception as e:`n            full_response = f"Lo siento, hubo un error técnico al procesar tu solicitud: {str(e)}"`n            message_placeholder.error(full_response)`n            `n    # Guardar respuesta asistente`n    st.session_state.messages.append({"role": "assistant", "content": full_response})
